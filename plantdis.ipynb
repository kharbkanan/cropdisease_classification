{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30daf4ab-c281-4c46-95c2-7cfa8790166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in a Jupyter cell\n",
    "from pathlib import Path\n",
    "ROOT = Path('plant')          # <-- your current root (change if different)\n",
    "CROP_CLASS_DIR = Path('data/crops_for_classification')  # created below\n",
    "MODEL_DIR = Path('models'); MODEL_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9f7901-0e97-4cbd-8745-ce852a416b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kashish Kharb\\plant\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17862cc4-c881-4697-93bc-f00e8a8f0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.18.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (1.65.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow==2.18.0) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.18.0 numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c7ae7d6-94e0-41e5-8902-8199e38dec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints: {}\n",
      "corn: {'corn_common_rust': 1192, 'corn_gray_leaf_spot': 513, 'corn_healthy': 1162, 'corn_northern_leaf_blight': 985}\n",
      "data: {'crops_for_classification': 0}\n",
      "models: {}\n",
      "potato: {'potato_early_blight': 1000, 'potato_healthy': 152, 'potato_late_blight': 1000}\n",
      "rice: {'rice_brown_spot': 613, 'rice_healthy': 1488, 'rice_leaf_blast': 977, 'rice_neck_blast': 1000}\n",
      "saved_models: {}\n",
      "src: {'.ipynb_checkpoints': 0, '__pycache__': 0}\n",
      "sugarcane: {'bacterial blight': 100, 'healthy': 100, 'red rot': 100}\n",
      "wheat: {'wheat_brown_rust': 902, 'wheat_healthy': 1116, 'wheat_yellow_rust': 924}\n",
      "No disease folders found in C:\\Users\\Kashish Kharb\\plant\\models\n"
     ]
    }
   ],
   "source": [
    "# list crops and disease folders + counts\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root = Path(r\"C:\\Users\\Kashish Kharb\\plant\")\n",
    "for crop in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
    "    disease_folders = [d for d in crop.iterdir() if d.is_dir()]\n",
    "    counts = {d.name: len(list(d.glob('*.jpg'))) for d in disease_folders}\n",
    "    print(f\"{crop.name}: {counts}\")\n",
    "\n",
    "# show a random sample image\n",
    "crops = [p for p in root.iterdir() if p.is_dir()]\n",
    "if crops:\n",
    "    crop = random.choice(crops)\n",
    "    diseases = [p for p in crop.iterdir() if p.is_dir()]\n",
    "    if diseases:\n",
    "        disease = random.choice(diseases)\n",
    "        img_files = list(disease.glob('*.jpg')) + list(disease.glob('*.jpeg')) + list(disease.glob('*.png'))\n",
    "        if img_files:\n",
    "            img_path = random.choice(img_files)\n",
    "            img = Image.open(img_path)\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{crop.name}/{disease.name}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No images found in {disease}\")\n",
    "    else:\n",
    "        print(f\"No disease folders found in {crop}\")\n",
    "else:\n",
    "    print(\"No crop folders found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8592c0e4-e797-4c43-a727-47bc8092ab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10661 images belonging to 5 classes.\n",
      "Found 2663 images belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 2s/step - accuracy: 0.8760 - loss: 0.3840 - val_accuracy: 0.9542 - val_loss: 0.1192\n",
      "Epoch 2/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 2s/step - accuracy: 0.9586 - loss: 0.1333 - val_accuracy: 0.9373 - val_loss: 0.1746\n",
      "Epoch 3/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 2s/step - accuracy: 0.9689 - loss: 0.0960 - val_accuracy: 0.9670 - val_loss: 0.0833\n",
      "Epoch 4/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 2s/step - accuracy: 0.9781 - loss: 0.0717 - val_accuracy: 0.9493 - val_loss: 0.1532\n",
      "Epoch 5/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 2s/step - accuracy: 0.9768 - loss: 0.0660 - val_accuracy: 0.9670 - val_loss: 0.0850\n",
      "âœ… Model and label map saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "BASE_DIR = Path(\"data/train\")\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# === Data generator ===\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# === Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_gen.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === Train ===\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# === Save model & label map ===\n",
    "model.save(MODEL_DIR / \"crop_classifier.keras\")\n",
    "joblib.dump(train_gen.class_indices, MODEL_DIR / \"crop_label_map.joblib\")\n",
    "\n",
    "print(\"âœ… Model and label map saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461d6f1b-6e58-4ff0-b96b-555391a14e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created crop-level dataset at: data\\crops_for_classification\n"
     ]
    }
   ],
   "source": [
    "# # create crop-level folders by copying (no change to original)\n",
    "# import shutil\n",
    "# from pathlib import Path\n",
    "\n",
    "# ROOT = Path(r\"C:\\Users\\Kashish Kharb\\plant\")\n",
    "# OUT = Path('data/crops_for_classification')\n",
    "# OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for crop in [p for p in ROOT.iterdir() if p.is_dir()]:\n",
    "#     target = OUT/crop.name\n",
    "#     target.mkdir(parents=True, exist_ok=True)\n",
    "#     for disease_folder in [d for d in crop.iterdir() if d.is_dir()]:\n",
    "#         for img in disease_folder.glob('*.jpg'):\n",
    "#             # copy using a unique name to avoid clashes\n",
    "#             dest = target / f\"{disease_folder.name}_{img.name}\"\n",
    "#             if not dest.exists():\n",
    "#                 shutil.copy(img, dest)\n",
    "\n",
    "# print(\"Created crop-level dataset at:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c30976-ce88-4b1f-a633-07842d614c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10661 images belonging to 10 classes.\n",
      "Found 2663 images belonging to 10 classes.\n",
      "Crop classes: {'.ipynb_checkpoints': 0, 'corn': 1, 'data': 2, 'models': 3, 'potato': 4, 'rice': 5, 'saved_models': 6, 'src': 7, 'sugarcane': 8, 'wheat': 9}\n"
     ]
    }
   ],
   "source": [
    "# # installs (uncomment if needed)\n",
    "# # !pip install tensorflow pillow matplotlib scikit-learn joblib opencv-python\n",
    "\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# IMG_SIZE = (224,224)\n",
    "# BATCH = 32\n",
    "\n",
    "# # for crop classifier (Stage A)\n",
    "# datagen_crop = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "#     zoom_range=0.1,\n",
    "#     horizontal_flip=True,\n",
    "#     validation_split=0.2\n",
    "# )\n",
    "\n",
    "# train_crop_gen = datagen_crop.flow_from_directory(\n",
    "#     'data/crops_for_classification', target_size=IMG_SIZE,\n",
    "#     batch_size=BATCH, subset='training', class_mode='categorical', shuffle=True)\n",
    "\n",
    "# val_crop_gen = datagen_crop.flow_from_directory(\n",
    "#     'data/crops_for_classification', target_size=IMG_SIZE,\n",
    "#     batch_size=BATCH, subset='validation', class_mode='categorical', shuffle=False)\n",
    "\n",
    "# print(\"Crop classes:\", train_crop_gen.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5365fd-42cb-4d59-9f45-9f7b927a6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 2s/step - accuracy: 0.9465 - loss: 0.2092 - val_accuracy: 0.9741 - val_loss: 0.0814\n",
      "Epoch 2/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 2s/step - accuracy: 0.9926 - loss: 0.0285 - val_accuracy: 0.9801 - val_loss: 0.0557\n",
      "Epoch 3/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 2s/step - accuracy: 0.9952 - loss: 0.0183 - val_accuracy: 0.9876 - val_loss: 0.0393\n",
      "Epoch 4/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 2s/step - accuracy: 0.9968 - loss: 0.0137 - val_accuracy: 0.9824 - val_loss: 0.0465\n",
      "Epoch 5/5\n",
      "\u001b[1m334/334\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 2s/step - accuracy: 0.9977 - loss: 0.0101 - val_accuracy: 0.9880 - val_loss: 0.0367\n",
      "Saved crop model and mapping.\n"
     ]
    }
   ],
   "source": [
    "# import joblib\n",
    "# from tensorflow.keras.applications import MobileNetV2\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# num_crops = train_crop_gen.num_classes\n",
    "# base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# x = layers.GlobalAveragePooling2D()(base.output)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# out = layers.Dense(num_crops, activation='softmax')(x)\n",
    "# model_crop = Model(base.input, out)\n",
    "\n",
    "# # freeze base for initial training\n",
    "# for layer in base.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# model_crop.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [ \n",
    "#     ModelCheckpoint(str(MODEL_DIR/'crop_classifier.keras'), save_best_only=True, monitor='val_loss'),\n",
    "#     EarlyStopping(patience=5, restore_best_weights=True)\n",
    "# ]\n",
    "\n",
    "# history = model_crop.fit(train_crop_gen, validation_data=val_crop_gen, epochs=5, callbacks=callbacks)\n",
    "\n",
    "# # save mapping: class name -> index\n",
    "# joblib.dump(train_crop_gen.class_indices, MODEL_DIR/'crop_label_map.joblib')\n",
    "# print(\"Saved crop model and mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb2acfc-c632-4735-995d-76984eec06fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil\n",
    "\n",
    "# remove unwanted Jupyter hidden folder (this causes the error)\n",
    "if os.path.exists('plant/.ipynb_checkpoints'):\n",
    "    shutil.rmtree('plant/.ipynb_checkpoints')\n",
    "    print(\"âœ… Removed .ipynb_checkpoints folder if it existed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "becceaa3-bd6a-4099-afa1-49b5f1318b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unwanted folder: C:\\Users\\Kashish Kharb\\plant\\.ipynb_checkpoints\n",
      "Removing unwanted folder: C:\\Users\\Kashish Kharb\\plant\\src\\.ipynb_checkpoints\n",
      "Removing unwanted folder: C:\\Users\\Kashish Kharb\\plant\\data\\crops_for_classification\\.ipynb_checkpoints\n",
      "âœ… Crops found: ['.ipynb_checkpoints', 'corn', 'data', 'models', 'potato', 'rice', 'saved_models', 'src', 'sugarcane', 'wheat']\n",
      "\n",
      "ğŸŒ¾ Training model for crop: .ipynb_checkpoints\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "âš ï¸ Skipping .ipynb_checkpoints: less than 2 disease classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: corn\n",
      "Found 3083 images belonging to 4 classes.\n",
      "Found 769 images belonging to 4 classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: data\n",
      "Found 10660 images belonging to 1 classes.\n",
      "Found 2664 images belonging to 1 classes.\n",
      "âš ï¸ Skipping data: less than 2 disease classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: models\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "âš ï¸ Skipping models: less than 2 disease classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: potato\n",
      "Found 1722 images belonging to 3 classes.\n",
      "Found 430 images belonging to 3 classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: rice\n",
      "Found 3264 images belonging to 4 classes.\n",
      "Found 814 images belonging to 4 classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: saved_models\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "âš ï¸ Skipping saved_models: less than 2 disease classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: src\n",
      "Found 0 images belonging to 1 classes.\n",
      "Found 0 images belonging to 1 classes.\n",
      "âš ï¸ Skipping src: less than 2 disease classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: sugarcane\n",
      "Found 240 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n",
      "\n",
      "ğŸŒ¾ Training model for crop: wheat\n",
      "Found 2355 images belonging to 3 classes.\n",
      "Found 587 images belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m148/148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 680ms/step - accuracy: 0.8620 - loss: 0.3673 - val_accuracy: 0.9761 - val_loss: 0.1104\n",
      "Epoch 2/5\n",
      "\u001b[1m148/148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 652ms/step - accuracy: 0.9626 - loss: 0.1218 - val_accuracy: 0.9898 - val_loss: 0.0483\n",
      "Epoch 3/5\n",
      "\u001b[1m148/148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 636ms/step - accuracy: 0.9711 - loss: 0.0908 - val_accuracy: 0.9813 - val_loss: 0.0645\n",
      "Epoch 4/5\n",
      "\u001b[1m148/148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 642ms/step - accuracy: 0.9762 - loss: 0.0779 - val_accuracy: 0.9813 - val_loss: 0.0594\n",
      "Epoch 5/5\n",
      "\u001b[1m148/148\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 639ms/step - accuracy: 0.9843 - loss: 0.0545 - val_accuracy: 0.9864 - val_loss: 0.0596\n",
      "âœ… Saved model and class map for wheat\n"
     ]
    }
   ],
   "source": [
    "# # ==============================\n",
    "# # ğŸ“˜ Crop Disease Classification\n",
    "# # ==============================\n",
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import joblib\n",
    "# from tensorflow.keras.applications import MobileNetV2\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # -----------------------------\n",
    "# # 1ï¸âƒ£ Define constants & folders\n",
    "# # -----------------------------\n",
    "# BASE_DIR = Path(r\"C:\\Users\\Kashish Kharb\\plant\")   # path where all crop folders exist\n",
    "# MODEL_DIR = Path(\"models\")\n",
    "# MODEL_DIR.mkdir(exist_ok=True)  # make 'models' folder if it doesn't exist\n",
    "\n",
    "# IMG_SIZE = (224, 224)\n",
    "# BATCH = 16\n",
    "\n",
    "# # -----------------------------\n",
    "# # 2ï¸âƒ£ Clean unwanted folders\n",
    "# # -----------------------------\n",
    "# # Remove any unwanted system folder (like .ipynb_checkpoints)\n",
    "# for subdir in BASE_DIR.rglob('.ipynb_checkpoints'):\n",
    "#     if subdir.is_dir():\n",
    "#         print(f\"Removing unwanted folder: {subdir}\")\n",
    "#         import shutil\n",
    "#         shutil.rmtree(subdir)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 3ï¸âƒ£ Identify all crop folders\n",
    "# # -----------------------------\n",
    "# crops = [p.name for p in BASE_DIR.iterdir() if p.is_dir()]\n",
    "# print(\"âœ… Crops found:\", crops)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4ï¸âƒ£ Train model per crop type\n",
    "# # -----------------------------\n",
    "# for crop in crops:\n",
    "#     print(f\"\\nğŸŒ¾ Training model for crop: {crop}\")\n",
    "\n",
    "#     crop_path = BASE_DIR / crop\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         rotation_range=20,\n",
    "#         width_shift_range=0.1,\n",
    "#         height_shift_range=0.1,\n",
    "#         zoom_range=0.1,\n",
    "#         horizontal_flip=True,\n",
    "#         validation_split=0.2\n",
    "#     )\n",
    "\n",
    "#     # Training and validation generators\n",
    "#     train_gen = datagen.flow_from_directory(\n",
    "#         crop_path,\n",
    "#         target_size=IMG_SIZE,\n",
    "#         batch_size=BATCH,\n",
    "#         subset='training',\n",
    "#         class_mode='categorical'\n",
    "#     )\n",
    "#     val_gen = datagen.flow_from_directory(\n",
    "#         crop_path,\n",
    "#         target_size=IMG_SIZE,\n",
    "#         batch_size=BATCH,\n",
    "#         subset='validation',\n",
    "#         class_mode='categorical'\n",
    "#     )\n",
    "\n",
    "#     # Skip if less than 2 classes\n",
    "#     if train_gen.num_classes < 2:\n",
    "#         print(f\"âš ï¸ Skipping {crop}: less than 2 disease classes.\")\n",
    "#         continue\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # 5ï¸âƒ£ Build model\n",
    "#     # -----------------------------\n",
    "#     base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "#     for layer in base.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     x = layers.GlobalAveragePooling2D()(base.output)\n",
    "#     x = layers.Dense(256, activation='relu')(x)\n",
    "#     out = layers.Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=base.input, outputs=out)\n",
    "#     model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # 6ï¸âƒ£ Callbacks\n",
    "#     # -----------------------------\n",
    "#     checkpoint_path = MODEL_DIR / f\"disease_{crop}.keras\"  # must end with .keras (new format)\n",
    "#     callbacks = [\n",
    "#         ModelCheckpoint(\n",
    "#             # str(checkpoint_path),========\n",
    "#             save_best_only=True,\n",
    "#             monitor='val_loss'\n",
    "#         ),\n",
    "#         EarlyStopping(patience=5, restore_best_weights=True)\n",
    "# ]\n",
    "# # 7ï¸âƒ£ Train model\n",
    "# model.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=callbacks)\n",
    "# # 8ï¸âƒ£ Save class mappings\n",
    "# joblib.dump(train_gen.class_indices, MODEL_DIR / f'{crop}_class_map.joblib')\n",
    "# print(f\"âœ… Saved model and class map for {crop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c269ed5f-f8a4-4b27-827e-6dcf03888abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# # Save your trained model\n",
    "# model.save(\"crop_disease_model.h5\")\n",
    "# print(\"âœ… Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "471b097f-dc7f-4ad1-ae6a-fc221102d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remedies loaded successfully: dict_keys(['corn_common_rust', 'corn_gray_leaf_spot', 'corn_healthy', 'corn_northern_leaf_blight', 'potato_early_blight', 'potato_healthy', 'potato_late_blight', 'rice_brown_spot', 'rice_healthy', 'rice_leaf_blast', 'rice_neck_blast', 'sugarcane_bacterial_blight', 'sugarcane_healthy', 'sugarcane_red_rot', 'wheat_brown_rust', 'wheat_healthy', 'wheat_yellow_rust'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"remedies.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Remedies loaded successfully:\", data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a22f5ef9-a6e7-4f75-8fd6-1513a43238d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets==8.1.2 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipywidgets==8.1.2) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipywidgets==8.1.2) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipywidgets==8.1.2) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipywidgets==8.1.2) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipywidgets==8.1.2) (3.0.16)\n",
      "Requirement already satisfied: decorator in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.2) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.2) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.1.2) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets==8.1.2) (1.16.0)\n",
      "Requirement already satisfied: jupyterlab_widgets in c:\\users\\kashish kharb\\new folder\\lib\\site-packages (3.0.16)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets==8.1.2\n",
    "!pip install jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1763ba7-b029-4c00-9b97-ca0b8c3e1819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded class labels: ['corn', 'potato', 'rice', 'sugarcane', 'wheat']\n",
      "âœ… Model loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccae35aaf0cc4b42861d005c0b08fc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value=(), accept='image/*', description='Upload'), Button(button_style='success', deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“· Step 1: Upload a leaf image\n",
      "ğŸ” Step 2: Click 'Predict Disease'\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import json\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "\n",
    "# === Folder paths ===\n",
    "TRAIN_DIR = Path(\"data/train\")  # path where 'corn', 'rice', 'wheat' folders are\n",
    "MODEL_DIR = Path(\"models\")\n",
    "\n",
    "# === Auto-load classes from folder names ===\n",
    "if TRAIN_DIR.exists():\n",
    "    classes = sorted(os.listdir(TRAIN_DIR))\n",
    "    print(f\"âœ… Loaded class labels: {classes}\")\n",
    "else:\n",
    "    print(\"âš ï¸ 'train' folder not found! Please check your path.\")\n",
    "    classes = []\n",
    "\n",
    "# === Load model (if exists) ===\n",
    "model_path = MODEL_DIR / \"crop_classifier.keras\"\n",
    "if model_path.exists():\n",
    "    model = load_model(model_path)\n",
    "    print(\"âœ… Model loaded successfully.\")\n",
    "else:\n",
    "    model = None\n",
    "    print(\"âš ï¸ No trained model found yet. Please train and save as 'models/crop_classifier.keras'.\")\n",
    "\n",
    "# === Load remedies ===\n",
    "try:\n",
    "    with open(\"remedies.json\", \"r\") as f:\n",
    "        remedies = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    remedies = {}\n",
    "    print(\"âš ï¸ remedies.json not found â€” remedies will be shown from default list.\")\n",
    "\n",
    "# === Default remedies if remedies.json missing ===\n",
    "default_remedies = {\n",
    "    \"corn\": \"ğŸŒ½ Remedy: Use resistant hybrids, rotate crops, and apply fungicides if needed.\",\n",
    "    \"rice\": \"ğŸŒ¾ Remedy: Maintain field drainage, avoid excess nitrogen, use certified seeds.\",\n",
    "    \"wheat\": \"ğŸŒ¿ Remedy: Remove infected leaves, apply fungicides, and use resistant varieties.\"\n",
    "}\n",
    "\n",
    "# === Widgets ===\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "predict_btn = widgets.Button(description=\"ğŸ” Predict Disease\", button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Display UI\n",
    "display(widgets.VBox([uploader, predict_btn, output]))\n",
    "\n",
    "# === Prediction function ===\n",
    "def process_upload(change=None):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if not uploader.value:\n",
    "            print(\"âš ï¸ Please upload an image first.\")\n",
    "            return\n",
    "\n",
    "        # --- Fix uploader handling ---\n",
    "        upload_data = uploader.value\n",
    "\n",
    "        # Handle different formats (tuple or dict)\n",
    "        if isinstance(upload_data, dict):\n",
    "            v = next(iter(upload_data.values()))\n",
    "        elif isinstance(upload_data, tuple) and len(upload_data) > 0 and isinstance(upload_data[0], dict):\n",
    "            v = upload_data[0]\n",
    "        else:\n",
    "            print(\"âš ï¸ Could not read uploaded image.\")\n",
    "            return\n",
    "\n",
    "        img_bytes = v['content']\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        display(img)\n",
    "\n",
    "        # Preprocess\n",
    "        img_resized = img.resize((224, 224))\n",
    "        img_array = np.array(img_resized) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        # Predict\n",
    "        pred = model.predict(img_array)\n",
    "        pred_class_index = np.argmax(pred)\n",
    "        confidence = np.max(pred) * 100\n",
    "        predicted_label = classes[pred_class_index]\n",
    "\n",
    "        # Remedy\n",
    "        remedy = remedies.get(predicted_label, default_remedies.get(predicted_label.lower(), \"No remedy found.\"))\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\nâœ… Prediction Complete!\")\n",
    "        print(f\"ğŸŒ¿ Predicted Crop: {predicted_label}\")\n",
    "        print(f\"ğŸ“Š Confidence: {confidence:.2f}%\")\n",
    "        print(f\"ğŸ’¡ Suggested Remedy: {remedy}\")\n",
    "\n",
    "# === Connect button to function ===\n",
    "predict_btn.on_click(process_upload)\n",
    "\n",
    "print(\"ğŸ“· Step 1: Upload a leaf image\\nğŸ” Step 2: Click 'Predict Disease'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4bd01f4-34e9-4fe7-9543-e6d4c7d5459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13324 files belonging to 5 classes.\n",
      "ğŸ§¾ Classes found: ['corn', 'potato', 'rice', 'sugarcane', 'wheat']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "train_dir = Path(\"./data/train\")\n",
    "\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"ğŸ§¾ Classes found:\", dataset.class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2e14e-928e-432f-8088-741ee0b1f7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
